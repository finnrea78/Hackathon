{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAAN UK F2F Guided Data Challenge --- No Code Worksheet** \n",
    "\n",
    "Welcome to the Roche UK DSC guided data challenge! If you are completely new to the world of data science we hope this challenge will give you the guidance and support you need to kick-start your data science journey. If you consider yourself a guru we hope this challenge provides some prompts for you to push your knowledge to the next level and gets you thinking about ways to advance your skill sets. \n",
    "\n",
    "This challenge has been written with the expectation of having _some_ prior python experience, though by no means extensive. If you are completely new to python, the `completed_code.ipynb` notebook will walk you directly through all the code you will need, or else provide links to where you can find it. So if you get stuck with any programming please don't hesitate to ask your team members and supervisors for help. We're all learning together! \n",
    "\n",
    "For our challenge today imagine you have been handed a large dataset of digitized images of FNAs (fine needle aspirate) of a breast mass and tasked to predict whether or not an image contains benign or malignant cancer cells. Being the savvy data analysts you all are you decide to employ a machine learning solution.\n",
    "\n",
    "By the time you finish this challenge, you will be able to :\n",
    "\n",
    "* Load the data, understand what is in it and clean datasets. \n",
    "* Visualise data \n",
    "* Split data using test/train split\n",
    "* Establish a baseline model using SVM, Decision Tree \n",
    "* How to improve a base machine learning model using Feature engineering Hyperparameter optimisation.\n",
    "* Program in Python!\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages \n",
    "\n",
    "This Python 3 environment comes with many helpful analytics libraries installed. It is defined by the [kaggle/python docker image](https://github.com/kaggle/docker-python). For example, here are several helpful packages to load in which we will have to use later on. \n",
    "Run the code below (by clicking run or pressing Shift+Enter).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import warnings # import warnings library\n",
    "warnings.filterwarnings('ignore') # ignore all warnings\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "To get started we first need to access our dataset which you should have been sent in the form of a csv file. Upload this file to colab by using the File tab on the left, and press the \"upload\" button, and specify `data.csv`. Then load it into a dataframe in pandas below using `pd.read_csv()`.\n",
    "\n",
    "\n",
    "Here are some websites that can help you import a csv file into python: \n",
    "\n",
    "[How to import a csv into python using pandas](https://datatofish.com/import-csv-file-python-using-pandas/)\n",
    "\n",
    "[How to read and write csv files with python](https://stackoverflow.com/questions/41585078/how-do-i-read-and-write-csv-files-with-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe called `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and viewing the data\n",
    "Now that we have access to the data we want to be able to see what is inside it, just to check everything is in order and ready to start working with. Since it's quite a large dataset we will save some time by only printing out the first and last 5 observations. Once you manage to do this look through the data and note down anything that looks suspicious. Note that in python that indexing will start from 0 (We start counting from 0 and not 1). We can also look at the statistics of each column in the data by calling the \"describe\" function.\n",
    "\n",
    "Helpful links:\n",
    "\n",
    "[How to view a portion of observations in a python dataset.](https://appdividend.com/2020/05/26/pandas-dataframe-head-method-in-python/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top 5 or ten elements in your dataframe.\n",
    "# This can be done by using the dataframe.head() function! How would you look at the last 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a statistical summary of your dataframe using dataframe.describe\n",
    "# Use the argument include = all to return mixed types (e.g. not just numeric columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief aside: working with pandas dataframes\n",
    "\n",
    "It's worthwhile trying to become comfortable with pandas dataframes, both in terms of selecting data, setting data, and various operations that you can perform on the data. In this way you can, as we shall soon see, remove rows or entire columns with missing values, create new features, and many other things. A link to a quick overview can be found in the official pandas docs: [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html). Have a read, and use the below code box to experiment with selecting and setting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can make a copy of your dataframe using\n",
    "# temp_data = data.copy()\n",
    "# This way you can experiment without risking losing your data, though any future work done on `temp_data`\n",
    "# will not effect `data` unless you do it to data too!\n",
    "\n",
    "# There are two ways to select data, by label and by index. Try them out here\n",
    "\n",
    "# How about selecting data by boolean values? Try selecting all rows with a radius_mean over 13.37\n",
    "\n",
    "# Can you create a new column called area_estimated where you estimate the area based off of the radius_mean\n",
    "# (assuming the area follows the equation for the area of a circle)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 things you hopefully should have noticed from the summaries:\n",
    "\n",
    "1)  <b>id</b> cannot be used for classification so should be removed (Discuss why this is with your team)\n",
    "\n",
    "2)  <b>diagnosis </b>  is the class label, and it is in a string format.\n",
    "\n",
    "3)  <b> Unnamed: 32 </b> feature includes Nan values and is not needed \n",
    "\n",
    "4)  We currently have no other knowledge about which features are necessary to keep.\n",
    "\n",
    "5) Data exists at different scales; some have a mean of less than 1, some are over 1000!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we do with this knowledge? \n",
    "\n",
    "Hint 1: Lets make our life easier by making the diagnosis a binary variable (1 for Malignant, 0 for Benign)\n",
    "\n",
    "[Hint 2: Some columns will need to be dropped](https://www.geeksforgeeks.org/how-to-drop-one-or-multiple-columns-in-pandas-dataframe/) \n",
    "\n",
    "Hint 3: One of these columns will need to be saved \n",
    "e.g. y = data.(column you want to keep)\n",
    "\n",
    "Hint 4: Some models really don't like it when input features are different orders of magnitude (others work perfectly well!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap the diagnosis column.\n",
    "# There are a few ways of doing this.\n",
    "# One \"pythonic\" way of doing it using using a dictionary and the map function\n",
    "# d = {\"M\": 1, \"B\": 0}\n",
    "# data[\"diagnosis\"].map(d)\n",
    "# How would you ensure this change is saved?\n",
    "\n",
    "\n",
    "# Make a new variable `y` containing just the diagnoses\n",
    "\n",
    "# Remove the \"id\" and \"Unnamed: 32\" columns\n",
    "\n",
    "# We will do the rescaling later on, once we've split the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the number of benign vs malignant cases \n",
    "\n",
    "Next we would like you to compare the number of benign vs malignant cancer cases using your saved variable and plot the result.\n",
    "\n",
    "[Working out index value using pandas](https://www.geeksforgeeks.org/python-pandas-index-value_counts/?ref=lbp)\n",
    "\n",
    "[How to create a seaborn.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn countplot to plot a histogram of the different cases\n",
    "\n",
    "# You can see how many of each value there are by using Series.value_counts()\n",
    "# (where series simply means you are referencing a particular column instead of a whole dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data\n",
    "\n",
    "The ground work is done for the data exploration!!\n",
    "\n",
    "To understand the hidden patterns, its always good to see big picture first and then dive deep into data. So lets begin analysis or visualization of patterns with our data and then move to the feature level understanding.... This will allow us to gain a better idea of which features have importance to us. In these next few steps you will be making box plots, heatmaps, violin plots and swarm plots!\n",
    "\n",
    "This data has 3 seperate feature sections: x_mean, x_se and x_worst\n",
    "\n",
    "Try and seperate the data into 3 seperate sections below. [Hint here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into X_mean, X_se and X_worst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of features\n",
    "\n",
    "Firstly we will be looking at the correlation( mutual relationship between two or more things) of features to one another.\n",
    "For this we will be using box plots and heat maps. The importance of this is so that we don't include two features that are strongly correlated\n",
    "to each other as this can off balance our machine learning algorithm that we will use later. To learn more about this click\n",
    " [here](https://towardsdatascience.com/why-exclude-highly-correlated-features-when-building-regression-model-34d77a90ea8e). Later we will just select 1 of the strongly correlated values. This next activity may be quite tricky so try and follow the given documentation and try and work out the step within your team and if all else fails your supervisor can help you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Boxplots \n",
    "\n",
    "Try to create a boxplot to compare two variables. For example radius_mean and texture_mean. [Here is the documentation for creating boxplots.](https://seaborn.pydata.org/generated/seaborn.boxplot.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Heatmaps \n",
    "\n",
    "Try to create heatmaps of the 3 different feature sections you have seperated. [Here is the documentation for creating heatmaps.](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap for x_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap for x_se "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap for x_worst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets identify the 10 most correlated features with the diagnosis label from the whole dataset (not the x_mean, x_worst, etc). You can use the [DataFrame.corr](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) function for this. Remember that we want the correlations with the diagnosis, which is the first column! Also bear in mind that correlation can be positive or negative. If we want both, it might be useful to take the absolute value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember how to index variables, and also be aware that you can sort a dataframe or series using Series.sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plots\n",
    "\n",
    "A [violin plot](https://mode.com/blog/violin-plot-examples/) is a hybrid of a box plot and a kernel density plot, which shows peaks in the data. It's used to visualize the distribution of\n",
    "numerical data. Unlike a box plot that can only show summary statistics, violin plots depict summary statistics and the density of each variable.\n",
    "Using these plots we can determine which features may be most important to determining benign and malignant cancer cells. Our data will need\n",
    " need to be standarized (converted to a common format) for this next step. \n",
    "\n",
    "[Hint here for standardization](https://www.askpython.com/python/examples/standardize-data-in-python)\n",
    "\n",
    "[Hint here for  violin plots](https://seaborn.pydata.org/generated/seaborn.violinplot.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plot for mean variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be easier to display results on a standardised dataset\n",
    "# data_dia = y\n",
    "# data = x_mean\n",
    "# data_standarized = (data - data.mean()) / (data.std()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plot for se variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plot for worst variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarm plots \n",
    "\n",
    "The last visualization we will look at is the [swarm plot](https://prvnk10.medium.com/swarm-plot-4728f52b688e). A swarmplot shows all the data points which helps to understand the distribution in a better manner. It also helps to understand how the data is distributed across a categorical attribute and how the continuous variable is varying within a category. This can be used to clearly define any differences between features and hopefully by the end of this step you can start to pick out what we can see to be our most important features. Again you will need to standardize the data for the best results.\n",
    "\n",
    "[Click here for seaborn documentation on swarm plots](https://seaborn.pydata.org/generated/seaborn.swarmplot.html) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swarm plot for mean variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swarm plot for se variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swarm plot for worst variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases,\n",
    " to improve the performance of the model. From our visulations we now need to pick some features run our models on.\n",
    " For more information on feature selection click [here](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/).\n",
    " We will first run feature selection for our mean cases \n",
    "\n",
    " We need to look at features that have strong correlations and group them together. \n",
    "\n",
    "For example radius_mean, perimeter_mean, area_mean are all highly correlated. Having grouped these variables together we only need to use perimeter_mean instead of all three variables.\n",
    "\n",
    "Try and group the other remaining variables and pick 4 other variables that you can use as prediction variables. Set up an array of the variables you have chosen as your prediction variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean','concavity_mean',\n",
    "# 'texture_se','perimeter_se','smoothness_se','compactness_se','symmetry_se','concavity_se',\n",
    "# 'texture_worst','perimeter_worst','smoothness_worst','compactness_worst','symmetry_worst','concavity_worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "“Prediction” refers to the output of an algorithm after it has been trained on a historical dataset and applied to new data when forecasting\n",
    "the likelihood of a particular outcome. In our case we will be splitting our data into training and testing data from\n",
    " \n",
    " which we will use our prediction variables to make judgements on whether or not an image shows benign or malignant cancer cells.\n",
    " We split the data into train and test to avoid overfitting where our machine learning model performs really well on data it has seen \n",
    " but fails when shown any new data. Underfitting can also occur where the train data is of poor quality.\n",
    "To read more about machine learning and predictions click [here](https://www.datarobot.com/wiki/prediction/).\n",
    "To read more on train/test click \n",
    "[here](https://towardsdatascience.com/how-to-split-a-dataset-into-training-and-testing-sets-b146b1649830#:~:text=The%20simplest%20way%20to%20split,the%20performance%20of%20our%20model.).\n",
    "\n",
    "Before we get started with splitting the data we need to install a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from sklearn.model_selection import GridSearchCV# for tuning parameter\n",
    "from sklearn.ensemble import RandomForestClassifier # for random forest classifier\n",
    "from sklearn import svm # for Support Vector Machine\n",
    "from sklearn import metrics # for the check the error and accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train/test \n",
    "\n",
    "Let's now split our dataset up into train and test datasets. A good ratio to split into is 80:20 train to test. The training dataset will \n",
    "be used to train the model to predict expected diagnosis from our selected features.\n",
    "The test dataset will be used to test how well the trained model can predict the correct diagnosis.\n",
    "\n",
    "[Follow this guide on how to split the data](https://towardsdatascience.com/splitting-a-dataset-e328dab2760a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 31)\n",
      "(114, 31)\n"
     ]
    }
   ],
   "source": [
    "# now split our data into train and test\n",
    "\n",
    "# we can check their dimension\n",
    "# print(train.shape)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split again into train_X and train_Y as well as test_X and test_Y. The X variable being the prediction variables and \n",
    "the Y variable being the variable we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data\n",
    "\n",
    "# Split the test data\n",
    "\n",
    "# n_features = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the rescaling. We will use Scikitlearn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for this purposes.\n",
    "\n",
    "The training data will be rescaled using `fit_transform`, while we will use `transform` for the test data to prevent data \"leakage\". `fit_transform` computes the mean and variance for the training dataset, then rescales the data according to these parameters. `transform` meanwhile, _reuses those parameters, instead of refitting them on the test dataset_.\n",
    "\n",
    "Why is this important? If we use `fit_transform` again, we will calculate a new mean and variance, and let our model learn about the new test dataset. We will therefore not get a good understanding of how the model is behaving on completely unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a standardscaler object\n",
    "# Call fit_transform on the training data to create a standardised training dataset\n",
    "\n",
    "# Call transform on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models \n",
    "\n",
    "### Random Forest Classifiers \n",
    "\n",
    "Now our data has been split we need to select some models we can train and use to predict. We will start off by looking at a random \n",
    "forest classifier. \n",
    "\n",
    "If the words machine learning scares you don't worry as it might not be as difficult as you first think. We will use the \n",
    " [sklearn libary](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to \n",
    "access these models so very little programming will be needed!\n",
    "\n",
    "A random forest classifier like its name implies, consists of a large number of individual decision trees that operate as an ensemble \n",
    "An error from an individual decision tree is negated as the results of the whole are averaged out to come to a correct decision.\n",
    "Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly \n",
    "different tree structures. Random forest takes advantage of this by allowing each individual tree to randomly sample from the dataset\n",
    " with replacement, resulting in different trees. This process is known as bagging. Please take some time to read \n",
    "[this post](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) for a more in depth look at random forest classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple random forest model\n",
    "# now fit our model for traiing data\n",
    "# predict for the test data\n",
    "\n",
    "# accuracy = metrics.accuracy_score(prediction,test_y)\n",
    "# err = 1 - accuracy\n",
    "# print(\"Prediction Accuracy = {0:4.2f}%\".format(accuracy*100))\n",
    "# print(\"Prediction Error = {0:4.2f}%\".format(err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "Random forests also provide feature importance in the `RandomForest.feature_importances_` array. Print the features with importance over some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importance_threshold = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot these feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_importance(n_features, importances, vars):\n",
    "#     fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\n",
    "\n",
    "#     # Identify the important features\n",
    "#     importance_threshold = 0.02\n",
    "#     idx = np.array(range(n_features))\n",
    "#     imp = np.where(importances >= importance_threshold)  # important features\n",
    "#     rest = np.setdiff1d(idx, imp)  # remaining features\n",
    "\n",
    "#     # Plot the important features and the rest on a bar chart\n",
    "#     plt.bar(idx[imp], importances[imp], alpha=0.65)\n",
    "#     plt.bar(idx[rest], importances[rest], alpha=0.45)\n",
    "\n",
    "#     # Print feature names on the bar chart\n",
    "#     for i, (feature, importance) in enumerate(zip(vars, importances)):\n",
    "#         if importance > importance_threshold:\n",
    "#             plt.text(i, 0.015, feature, ha='center', va='bottom', rotation='vertical', fontsize=16, fontweight='bold')\n",
    "#             print('[{0}] {1} (score={2:4.3f})'.format(i, feature, importance))\n",
    "#         else:\n",
    "#             plt.text(i, 0.01, feature, ha='center', va='bottom', rotation='vertical', fontsize=16, color='gray')\n",
    "        \n",
    "#     # Finish the plot    \n",
    "#     fig.axes[0].get_xaxis().set_visible(False)\n",
    "#     plt.ylabel('Feature Importance Score', fontsize=16)\n",
    "#     plt.xlabel('Features for Breast Cancer Diagnosis', fontsize=16)\n",
    "#     plt.show()\n",
    "\n",
    "# Use the above function to plot feature imporatances for our random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Prediction \n",
    "\n",
    "Now lets try making an individual prediction based off the model we have just set up. In the space below create an individual example \n",
    " from your test data for both X and Y variables. They should be from the same index in your data.\n",
    "Then use your model to make a prediction. And see if it is correct! Try lots of different individual records and see how many predictions \n",
    " are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (Support Vector Model)\n",
    "\n",
    "An SVM or Support Vector Model finds a hyperplane (a line) in an N-dimensional space (N — the number of features) that \n",
    "distinctly classifies the data points. For a high level overview of SVM please read this \n",
    " [article](https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989).  \n",
    "\n",
    " [For the relevant code for an SVM click here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    " The same as before try setting up the model and running the model on individual observations in your test data. \n",
    " Are the number of correct observations higher for this model or lower?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a SVM with a *linear kernel*. This will allow us to look at feature importances.\n",
    "\n",
    "\n",
    "# print(\"Prediction Accuracy = {0:4.2f}%\".format(accuracy*100))\n",
    "# print(\"Prediction Error = {0:4.2f}%\".format(err*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our prediction on a particular individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances in SVMs using linear kernels can bo obtained from svm.coef_\n",
    "# Note that you might have to change the dimensions of the coefficients using array.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Prediction Accuracy \n",
    " \n",
    "\n",
    "Now you have hopefully been able to get a working machine learning model up and running you may have noticed that some predictions have\n",
    " been inaccurate. If we had to use this in a real medical scenario this would be a disaster! Thankfully there are many ways we can \n",
    " improve our machine learning models to make sure they provide accurate diagnoses. Here are a couple:\n",
    "\n",
    "* Using larger and more complex data in your training set. \n",
    "* Testing multiple algorithms. (You have already done this!) \n",
    "\n",
    "### [Hyperparameterisation](https://www.jeremyjordan.me/hyperparameter-tuning/)\n",
    " \n",
    "\n",
    "Hyperparameters are the parameters in machine learning models that determine how they work. These parameters can include things like \n",
    "the number of layers in a deep neural network, or how many trees there should be in an ensemble model. You usually need to adjust\n",
    " these hyperparameters yourself because they aren’t automatically set when you train your model. In true machine learning fashion, \n",
    " we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically and thus this \n",
    " process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "### [Oversampling](https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf)\n",
    "\n",
    "Random Oversampling includes selecting random examples from the minority class with replacement and supplementing the training data \n",
    "with multiple copies of this instance, hence it is possible that a single instance may be selected multiple times. For Machine Learning \n",
    "algorithms affected by skewed distribution, such as artificial neural networks and SVMs, this is a highly effective technique.\n",
    "\n",
    "###  [Ensemble methods.](https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c)\n",
    " \n",
    "\n",
    "Another approach is to use an ensemble method, which combines two or more algorithms together into one model. Ensembles are often \n",
    " more accurate than any individual algorithm because they leverage the strengths of each and compensate for their weaknesses.\n",
    "\n",
    "In other words, if you combine multiple weak learners (i.e., models that perform poorly on their own) into one ensemble, you can get \n",
    " a stronger learner (i.e., a model that performs well as an individual).\n",
    "\n",
    "### [Cross validation.](https://machinelearningmastery.com/k-fold-cross-validation/) \n",
    " \n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. \n",
    " As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k \n",
    " in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. \n",
    "That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions \n",
    " on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic \n",
    "estimate of the model skill than other methods, such as a simple train/test split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed the DSC data challenge 2022!\n",
    "\n",
    "We hope you have been able to learn something new today that can potentially be used in hackathons that you may join in future and \n",
    "has opened up the world of data science to you! We have outlined a basic pipeline for building and applying machine learning models, but this is just the basics --- much more can be done. We've also included some extra things for you to try!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Model selection\n",
    "\n",
    "We've tried out a random forest classifier, as well as an SVM... what about another kind of model? What is the best way of figuring out what model to use? Well, one way is just to try lots out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/skatesa/opt/miniconda3/lib/python3.8/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/skatesa/opt/miniconda3/lib/python3.8/site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in /Users/skatesa/opt/miniconda3/lib/python3.8/site-packages (from xgboost) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of these are from SKLearn, however xgboost is also really good!\n",
    "# classifiers = {\n",
    "#     \"XGBClassifier\": XGBClassifier(),\n",
    "#     \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "#     \"LogisticRegression\": LogisticRegression(),\n",
    "#     \"PassiveAggressiveClassifier\": PassiveAggressiveClassifier(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will loop over each model, fit the data using `train_X` and `train_y`, generate predictions using `test_X` and `test_y` and calculate the accuracy across multiple rounds of cross-validation to get an unbiased estimate of each models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# df_models = pd.DataFrame(columns=['model', 'run_time', 'accuracy', 'accuracy_cv'])\n",
    "\n",
    "# Loop over all models in `classifiers`, train your model, then predict diagnoses on the test set.\n",
    "# Then run 10-fold cross validation using cross_val_score with scoring = \"accuracy\"\n",
    "# Save all the results to df_models so we can see them afterwards!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Hyperparameter tuning\n",
    "\n",
    "To finish our challenge why not try some hyperparameter tuning yourself. \n",
    " [Check out this article for a guide on this.](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
    "\n",
    "Our best performing models were the logistic regression and the XGBoost classifier. These are just the models with default parameters though. Do we have any more room to improve if we optimise them?\n",
    "\n",
    "The first step is to understand what parameters are available, and which can be optimised. You can do this by printing `model.get_params()`, however you may need to look at the documentation for your particular model to understand how it can be tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = XGBClassifier()\n",
    "# Fit the classifier here\n",
    "\n",
    "\n",
    "# Look at the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take a subset of the hyperparameters and add them to a dictionary and assign this to a param_grid.\n",
    "\n",
    "We can then define the model and configure the [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) function to test each unique combination of hyperparameters and record the accuracy on each iteration. After going through the whole batch, the optimum model parameters can be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = dict(\n",
    "#     n_jobs = [16],\n",
    "#     learning_rate = [0.1, 0.5],\n",
    "#     objective = ['binary:logistic'],\n",
    "#     max_depth = [int(x) for x in np.linspace(1, 21, num = 11)], \n",
    "#     n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)],\n",
    "#     subsample = [0.2, 0.8, 1.0],\n",
    "#     gamma = [0, 0.05, 0.5],\n",
    "#     scale_pos_weight = [0, 1],\n",
    "#     reg_alpha = [0, 0.5],\n",
    "#     reg_lambda = [1, 0],\n",
    "# )\n",
    "\n",
    "# model = XGBClassifier(random_state=1, verbosity=1)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Call RandomizedSearchCV on model using param_grid\n",
    "\n",
    "# best_model = random_search.fit(train_X, train_y)\n",
    "# print('Optimum parameters', best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = XGBClassifier(\n",
    "#     # Set all the parameters we found from above here\n",
    "# )\n",
    "\n",
    "# Now fit the model and compare the performance with our earlier model\n",
    "# accuracy = metrics.accuracy_score(y_pred, test_y)\n",
    "# err = 1 - accuracy\n",
    "\n",
    "# print(\"Prediction Accuracy = {0:4.2f}%\".format(accuracy*100))\n",
    "# print(\"Prediction Error = {0:4.2f}%\".format(err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gotten our prediction accuracy up to 98%! That's definitely an improvement on the default parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Ensemble methods\n",
    "\n",
    "We've tried out using a _single_ model, but what happens when multiple models join forces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = {\n",
    "#     \"XGBClassifier\": XGBClassifier(),\n",
    "#     \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "#     \"LogisticRegression\": LogisticRegression(),\n",
    "#     \"PassiveAggressiveClassifier\": PassiveAggressiveClassifier(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "# }\n",
    "\n",
    "# def fit(classifiers, X, y):\n",
    "#     for model, classifier in classifiers.items():\n",
    "#         classifier.fit(X, y)\n",
    "#     return classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set aside some data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_X, val_X, new_train_y, val_y = train_test_split(train_X, train_y, test_size=0.25)        # Set aside 25% of data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function we defined above to fit all of our classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we have 8 individual \"base\" classifiers, each test example will end up with 8 different predictions, each one corresponding to an associated base classifier.\n",
    "\n",
    "We are not just interested in the diagnosis itself, but also the probabilities for the diagnosis. Lets make a function that returns the predictions for each classifier, and allows us to specify whether we want the class label or the probability as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_individual(X, classifiers, prob=False):\n",
    "    n_classifiers = len(classifiers.keys())\n",
    "    n_samples = X.shape[0] \n",
    "\n",
    "    y = np.zeros((n_samples, n_classifiers))\n",
    "    for i, (model, classifier) in enumerate(classifiers.items()):\n",
    "        if prob:\n",
    "            y[:, i] = classifier.predict_proba(X)[:, 1]  \n",
    "        else:\n",
    "            y[:, i] = classifier.predict(X)              \n",
    "    return y\n",
    "\n",
    "# Test with prob=False\n",
    "\n",
    "# Sanity check that the output has the same number of examples as the test dataset, and 8 estimates, and is 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are a few different ways of combining these. We will try a few out here:\n",
    "\n",
    "1) Majority vote: simply pick the most common label from each set of the predictions\n",
    "\n",
    "2) Accuracy weighting: higher performing classifiers are given greater weight in the ensemble\n",
    "\n",
    "3) Entropy weighting: similar to above, except entropy is used in deciding the weighting (where entropy is a form of uncertainty)\n",
    "\n",
    "We can write some helper functions to apply these methods. Have a read of the code below and make sure you understand what they are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import mode\n",
    "\n",
    "# def combine_using_majority_vote(X, classifiers):\n",
    "#     y_individual = predict_individual(X, classifiers, prob=False)\n",
    "#     y_final = mode(y_individual, axis=1)\n",
    "#     return y_final[0].reshape(-1, )\n",
    "\n",
    "# def combine_using_accuracy_weighting(X, classifiers, Xval, yval):\n",
    "#     n_classifiers = len(classifiers)\n",
    "#     yval_individual = predict_individual(Xval, classifiers, prob=False)\n",
    "    \n",
    "#     wts = [metrics.accuracy_score(yval, yval_individual[:, i]) \n",
    "#        for i in range(n_classifiers)] \n",
    "#     wts /= np.sum(wts)\n",
    "\n",
    "#     ypred_individual = predict_individual(X, classifiers, prob=False)\n",
    "#     y_final = np.dot(ypred_individual, wts) \n",
    "\n",
    "#     return np.round(y_final)\n",
    "\n",
    "# # For entropy weighting first define how to calculate entropy\n",
    "# def entropy(y):\n",
    "#     _, counts = np.unique(y, return_counts=True) \n",
    "#     p = np.array(counts.astype('float') / len(y))\n",
    "#     ent = -p.T @ np.log2(p) # @ is the matrix multiplication operator\n",
    "\n",
    "#     return ent\n",
    "\n",
    "# def combine_using_entropy_weighting(X, classifiers, Xval, yval):\n",
    "#     n_classifiers = len(classifiers)\n",
    "#     yval_individual = predict_individual(Xval, classifiers, prob=False)\n",
    "    \n",
    "#     wts = [1/entropy(yval_individual[:, i]) \n",
    "#            for i in range(n_classifiers)]\n",
    "#     wts /= np.sum(wts)\n",
    "\n",
    "#     ypred_individual = predict_individual(X, classifiers, prob=False)\n",
    "#     y_final = np.dot(ypred_individual, wts)\n",
    "    \n",
    "#     return np.round(y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the three different functions to compare their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do something called meta-learning, where instead of carefully designing a combination function to combine predictions, we will learn a combination function over the individual predictions. That is, the predictions of the base estimators are given as inputs to a second-level learning algorithm. Thus, rather than designing one ourselves, we will learn a second-level meta-classification function!\n",
    "\n",
    "Stacking is the most common meta-learning method and gets its name because it stacks a second classifier on top of its base estimators. The general stacking procedure has two steps:\n",
    "\n",
    "1) level 1: fit base estimators on the training data; this step is the same as before and aims to create a diverse, heterogeneous set of base classifiers.\n",
    "2) level 2: construct a new data set from the output of the base classifiers, which become meta-features; meta-features can either be the predictions or the probability of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets again define our models first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = {\n",
    "#     \"XGBClassifier\": XGBClassifier(),\n",
    "#     \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "#     \"LogisticRegression\": LogisticRegression(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then lets define some helper functions to allow us to use a second level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our classifiers\n",
    "# def fit_stacking(level1_classifiers, level2_classifier, X, y, use_probabilities=False):\n",
    "\n",
    "#     fit(level1_classifiers, X, y)\n",
    "    \n",
    "#     X_meta = predict_individual(X, classifiers=level1_classifiers, prob=use_probabilities)\n",
    "    \n",
    "#     level2_classifier.fit(X_meta, y)\n",
    "\n",
    "#     final_model = {'level-1': level1_classifiers, \n",
    "#                    'level-2': level2_classifier, \n",
    "#                    'use-prob': use_probabilities}\n",
    "    \n",
    "#     return final_model\n",
    "\n",
    "# # Predict using the classifiers\n",
    "# def predict_stacking(X, stacked_model):\n",
    "#     level1_classifiers = stacked_model['level-1']\n",
    "#     use_probabilities = stacked_model['use-prob']\n",
    "\n",
    "#     X_meta = predict_individual(X, classifiers=level1_classifiers, prob=use_probabilities)\n",
    "\n",
    "#     level2_classifier = stacked_model['level-2']\n",
    "#     y = level2_classifier.predict(X_meta)\n",
    "    \n",
    "#     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try it out! Logistic Regression is a common second level estimator, but feel free to experiment with different ones yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your meta classifier here\n",
    "\n",
    "# The call the fit_stacking function and predict stacking function to check their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, the accuracy actually is worse than our previous methods, however this is just a basic attempt. We can combine parameter optimisation, incorporate cross validation into the stacking procedure, and many more things to try and increase the accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, ensemble learning can be a powerful tool to augment your machine learning toolbox!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba885f589dc37093157b57de134f21c9af3b9a4cc1cfc1274c381eed94b9b2e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
